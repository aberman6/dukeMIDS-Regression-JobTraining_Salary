---
title: "Team Project 1"
author: "Allison Young and Anna Berman"
date: "10/24/2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE, warning=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd('/Users/annaberman/Desktop/702 Modeling/Assignments/Team Project #1')
library(ggplot2)
library(dplyr)
library(gridExtra)
library(arm)
library(MASS)
library(pROC)
library(kableExtra)
```

## Data Overview 

In the 1970s, researchers in the United States ran several randomized experiments intended to evaluate public policy programs. One of the most famous experiments is the National Supported Work Demonstration (NSWD), in which researchers wanted to assess whether or not job training for disadvantaged workers had an effect on their wages. Eligible workers were randomly assigned either to receive job training or not to receive job training. Since this is a randomized experiment, we can make causal claims about the effect of job training on wages for this population of workers. 

We analyze a subset of the data from the NSWD. These and other data were originally analyzed in a highly influential paper by the economist Robert Lalonde. The reference for the study is Lalonde, R. J. (1986), Evaluating the econometric evaluations of training programs with experimental data, The American Economic Review, 76, 604 - 620.

We will use linear and logistic regression modeling to answer the following questions of interest.

* Is there evidence that workers who receive job training tend to earn higher wages than workers who do not receive job training? What is a likely range for the effect of training? Is there any evidence that the effects differ by demographic groups? Are there other interesting associations with wages that are worth mentioning?

* Is there evidence that workers who receive job training tend to be more likely to have positive (non-zero) wages than workers who do not receive job training? What is a likely range for the effect of training? Is there any evidence that the effects differ by demographic groups? Are there other interesting associations with positive wages that are worth mentioning?

A summary of the dataset used in both our linear and logisitic regessions is summarized below:

```{r data, echo = FALSE}
# Load the data
lalonde <- read.csv('lalondedata.txt') %>%
    mutate(treat = as.factor(treat)) %>%
    mutate(treat = factor(treat, levels = c(0, 1))) %>%
    # mean centering
    mutate(re78c = re78 - mean(re78),
           re75c = re75 - mean(re75),
           re74c = re74 - mean(re74),
           agec = age - mean(age)) %>%
    # Employed in 1978
    mutate(employed78 = ifelse(re78 > 0, 1, 0),
           employed75 = ifelse(re75 > 0, 1, 0),
           employed74 = ifelse(re74 > 0, 1, 0)) %>%
    # educ
    mutate(educ.bin = ifelse(educ < 9, 'MS or less',
                             ifelse(educ < 12, 'Some HS',
                                    ifelse(educ == 12, 'HS', 'More than HS')))) %>%
    mutate(educ.bin = factor(educ.bin, levels = c('HS', 'MS or less', 'Some HS', 'More than HS'))) %>%
    mutate(educ.bin2 = ifelse(educ < 9, 'MS or less', 'Some HS +')) %>%
    mutate(educ.bin2 = factor(educ.bin2, levels = c('Some HS +', 'MS or less'))) %>%
    # age polynomial
    mutate(age2 = agec**2, age3 = agec**3) 

summary(lalonde)
```



# Linear Regression
## Exploratory Data Analysis

For concern of multicollinearity, we cannot include both nodegree and education in our model (nodegree is, in essence, a binned version of education with 0 being over 12 years of education and 1 being less than 12 years of education). We were originally concerned with including both 1974 salary (re74) and 1975 salary (re75), however, the correlation between these two variables is only `r round(cor(lalonde$re74, lalonde$re75),2)` which low enough to allow both salary variables as predictors in our model. No other variables had high enough correlation to be a multicollinearity concern.

A plot of each predictor in relation to our outcome variable, 1978 salary is below.

```{r exploratory.linear, echo = FALSE, fig.height=5, message = FALSE}
# TREAT
e1 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = treat, y = re78)) + 
    ylim(c(0,60000)) + 
    ggtitle('Treatment') +
    ylab('1978 Salary')

# AGE
e2 <- ggplot(lalonde) +
    geom_point(mapping = aes(x = age, y = re78)) + 
    ylim(c(0,61000)) + 
    ggtitle('Age (years)') +
    ylab('1978 Salary')


# EDCU.BIN
e3 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = educ.bin, y = re78)) + 
    ylim(c(0,61000)) + 
    ggtitle('Binned Education') +
    ylab('1978 Salary')


# BLACK
e4 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = as.factor(black), y = re78)) + 
    ylim(c(0,61000)) + 
    ggtitle('Black') +
    ylab('1978 Salary')


# HISPAN
e5 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = as.factor(hispan), y = re78)) + 
    ylim(c(0,61000)) + 
    ggtitle('Hispanic') +
    ylab('1978 Salary')


# MARRIED
e6 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = as.factor(married), y = re78)) + 
    ylim(c(0,61000)) + 
    ggtitle('Married') +
    ylab('1978 Salary')


# RE74
e7 <- ggplot(lalonde) +
    geom_point(mapping = aes(x = re74, y = re78)) + 
    ylim(c(0,61000)) + 
    xlim(c(0,61000)) +
    ggtitle('1974 Salary') +
    ylab('1978 Salary')


# RE75
e8 <- ggplot(lalonde) +
    geom_point(mapping = aes(x = re75, y = re78)) + 
    ylim(c(0,61000)) + 
    xlim(c(0,61000)) + 
    ggtitle('1975 Salary') +
    ylab('1978 Salary')

# NODEGREE
e9 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = as.factor(nodegree), y = re78)) + 
    ylim(c(0,61000))+ 
    ggtitle('No Degree') +
    ylab('1978 Salary')


# Plot all the plots together
grid.arrange(e1, e2, e3, e4, e5, e6, e7, e8, e9,
             top = 'Predictors vs. 1978 Salary')
```

 
## Model Selection

Through a series of modeling fittings, we examined a variety of logistic models to answer the question, 'Is there evidence that workers who receive job training tend to earn higher wages than workers who do not receive job training?'. We evaluated each model based on R-squared value and whether addition variables and interactions resulted in a significant or near-significant nested F test results.
 
We attempted logging our outcome variable (1978 salary (re78)), logging 1974 and 1975 salaries, using nodegree as opposed to education, using education as a continuous variable as well as a binned factor variable. We also looked at potential interaction effects between treatment and education, treatment and black, treatment and hispanic, and treatment and age (see appendix Fig. 1 for plots of potential interaction effects). Additionally, we used mean-centered continuous variables to aid in interpretation.

Before we finalized our model selection we examined the residuals and influential points. The residuals of this model are normally distributed and have constant variance therefore fitting our assumptions of linear regression (see appendix). The most influential points in our model were determined to be corner cases and did not call for alteration of our final model. (For further details on our model's redisuals and influential points, see appendix).

Ultimately, we selected the model summarized below. 

```{r model.linear, echo = FALSE}
final_linear_fit <- lm(re78 ~ treat + agec + educ.bin + black + hispan + married + re74c + re75c, data = lalonde)
summary(final_linear_fit)

coeff <- cbind(summary(final_linear_fit)$coefficients[, c(1,2)],
               confint(final_linear_fit))
coeff <- data.frame(coeff) 
names(coeff) <- c('Estimate', 'Std.Error', '2.5%', '97.5%')
```

## Interpretation

Our model has an R-squared of `r round(summary(final_linear_fit)$r.squared,2)`. In other words, our model explains `r round(summary(final_linear_fit)$r.squared,2)*100`% of the variance 1978 salary.

**Intercept**: For non-black, non-hispanic, un-married individuals of average age, average 1974 and 1975 salaries, with High School only education, who did not recieve treatment, we estimate the average salary in 1978 to be \$`r round(coeff['(Intercept)','Estimate'],2)` (95% CI: \$`r round(coeff['(Intercept)','2.5%'],2)`, \$`r round(coeff['(Intercept)','97.5%'],2)`). 

**Treatment**: Holding all else constant, individuals who participated in the treatment are estimated to have average 1978 salaries increased by \$`r round(coeff['treat','Estimate'],2)` (95% CI: \$`r round(coeff['treat','2.5%'],2)`, \$`r round(coeff['treat','97.5%'],2)`).

**Age**: Holding all else constant, for each 10 years an individual ages on average we estimate his salary to increase by \$`r round(coeff['agec','Estimate']*10,2)` (95% CI: \$`r round(coeff['agec','2.5%']*10,2)`, \$`r round(coeff['agec','97.5%']*10,2)`). Given that this confidence interval includes 0, we are not confident that there is a meaningful effect of age on 1978 salary.

**Education**: Holding all else contant, for an individual with:

* Less than a middle school education: we estimate avergage 1978 salary to be \$`r -round(coeff['educ.binMS or less','Estimate'],2)` less (95% CI: \$`r round(coeff['educ.binMS or less','2.5%'],2)`, \$`r round(coeff['educ.binMS or less','97.5%'],2)`). 

* Some high school education: we estimate avergage 1978 salary to be \$`r round(coeff['educ.binSome HS','Estimate'],2)` more (95% CI: \$`r round(coeff['educ.binSome HS','2.5%'],2)`, \$`r round(coeff['educ.binSome HS','97.5%'],2)`). Given that this confidence interval includes 0, we are not confident that there is a meaningful effect of some high school compared to completion of high school on 1978 salary.

* More than a high school education: we estimate avergage 1978 salary to be \$`r round(coeff['educ.binMore than HS','Estimate'],2)` more (95% CI: \$`r round(coeff['educ.binMore than HS','2.5%'],2)`, \$`r round(coeff['educ.binMore than HS','97.5%'],2)`).

**Married**: Holding all else constant, for married individuals we estimate average 1978 salaries to be \$`r round(coeff['married','Estimate'],2)` more (95% CI: \$`r round(coeff['married','2.5%'],2)`, \$`r round(coeff['married','97.5%'],2)`). Given that this confidence interval includes 0, we are not confident that there is a meaningful effect of blackness on 1978 salary.

**Black**: Holding all else constant, for black individuals we estimate average 1978 salaries to be \$`r -round(coeff['black','Estimate'],2)` less (95% CI: \$`r round(coeff['black','2.5%'],2)`, \$`r round(coeff['black','97.5%'],2)`). Given that this confidence interval includes 0, we are not confident that there is a meaningful effect of blackness on 1978 salary.

**Hispanic**: Holding all else constant, for black individuals we estimate average 1978 salaries to be \$`r round(coeff['hispan','Estimate'],2)` more (95% CI: \$`r round(coeff['hispan','2.5%'],2)`, \$`r round(coeff['hispan','97.5%'],2)`). Given that this confidence interval includes 0, we are not confident that there is a meaningful effect of hispanic ethnicity on 1978 salary.

**1974 Salary**: Holding all else constant, for each $1,000 an individual made in 1974, on average we estimate his 1978 salary to be \$`r round(coeff['re74c','Estimate']*1000,2)` higher (95% CI: \$`r round(coeff['re74c','2.5%']*1000,2)`, \$`r round(coeff['re74c','97.5%']*1000,2)`). 

**1975 Salary**: Holding all else constant, for each $1,000 an individual made in 1975, on average we estimate his 1978 salary to be \$`r round(coeff['re75c','Estimate']*1000,2)` higher (95% CI: \$`r round(coeff['re75c','2.5%']*1000,2)`, \$`r round(coeff['re75c','97.5%']*1000,2)`). 

## Discussion

Because this is a randomized control trial, we can say treatment may result in increased salaries. However the effect size of treatment may be small.

The effect of treatment on salary is confounded by additional factors that, when isolated, also explain differences in salary in this data set. These factors include age, ethnicity, education, marital status, and previous salary.

Some factors may or may not have an effect on salary. The factors that fall into this category are being black, being hispanic, and being married. This is evident because, when isolated, the cofidence intervals of the model coefficients include zero.

From our data exploration, it appears the 1974 salary of individuals is more representative of earning potential in 1978 compared to 1975, and thus a greater predictor factor of 1978 salary in our final model.

It is unclear whether having a high school diploma differs significantly from having some high school education when it comes to salary in 1978. However, it is clear that having less than a high school education results in a lower salary, and having more than a high school education results in a higher salary.

### Limitations

Our model has an R-squared of `r round(summary(final_linear_fit)$r.squared,2)`. In other words, our model explains `r round(summary(final_linear_fit)$r.squared,2)*100`% of the variance 1978 salary. It seems that we are missing variables in our model that would explain additional variation in 1978 salary, therefore more research is needed to fully understand the relationship between job training programs and salary and the mediating variables in this relationship.

Doesn't work for wealthy or older individuals (Why older?)
Also black
Outliers



# Logistic Regression

## Exploratory Data Analysis

In terms of mulicollinearity, the same reasoning from our linear regression applies to our logistic regression. Therefore our only restriciton is a choice between either education or nodegree. 

A plot of each predictor in relation to our outcome variable, employment in 1978 is below (employment being defined as salary above 0).

```{r log.explore, echo = FALSE}
# Create table of means
treat <- tapply(lalonde$employed78, lalonde$treat, mean)
educ.bin2 <- tapply(lalonde$employed78, lalonde$educ.bin2, mean) 
black <- tapply(lalonde$employed78, lalonde$black, mean) 
hispan <- tapply(lalonde$employed78, lalonde$hispan, mean) 
married <- tapply(lalonde$employed78, lalonde$married, mean) 
nodegree <- tapply(lalonde$employed78, lalonde$nodegree, mean) 

# Print table
rbind(treat, educ.bin2, black, hispan, married, nodegree) %>%
    round(2) %>%
    kable(caption = 'Average employed \'78 Cases by predictor')

par(mfrow = c(2,2))
#EDUC
binnedplot(x = lalonde$educ, y = lalonde$employed78,
           xlab = 'Education', ylab = 'employed \'78 Cases',
           ylim = c(0,1),
           main = 'Education')

# AGE
binnedplot(x = lalonde$age, y = lalonde$employed78,
           xlab = 'Age', ylab = 'employed \'78 Cases',
           ylim = c(0,1),
           main = 'Age')


# RE74
binnedplot(x = lalonde$re74, y = lalonde$employed78,
           xlab = 'Re74', ylab = 'employed \'78 Cases',
           ylim = c(.6,1),
           main = '1974 Salary')

# RE75
binnedplot(x = lalonde$re75, y = lalonde$employed78,
           xlab = 'Re75', ylab = 'employed \'78 Cases',
           ylim = c(.6,1),
           main = '1975 Salary')
par(mfrow = c(1,1))
```


## Model Selection

We examined a variety of linear models to answer the question, 'Is there evidence that workers who receive job training tend to be more likely to have positive (non-zero) wages than workers who do not receive job training?'. We evaluated each model based on the area under the curve (AUC) and whether addition variables and interactions resulted in a significant or near-significant change in deviance tests.

We attempted binning education in multiple ways, nonzero 1974 and 1975 variables. We also examoined potential interactions between treatment and previous salaries as well as interactions between treatment and level of education (see appendix for examination of interaction effects). Additionally, we used mean-centered continuous variables to aid in interpretation.

Before we finalized our model selection we examined the residuals and influential points. The residuals of this model fit our assumptions of logistic regression (see appendix). 

The most influential points in our model were determined to be corner cases and did not call for alteration of our final model. (For further details on our model's redisuals and influential points, see appendix).

Ultimately, we selected the model summarized below. 

```{r model.log, echo = FALSE, fig.height=3}
# Final fit
final_log_fit <- glm(employed78 ~ treat*employed74 + agec + married + black + hispan + 
                         educ.bin2 + re75c, 
                     data = lalonde, family = binomial)
summary(final_log_fit)

roc <- roc(lalonde$treat, fitted(final_log_fit))
plot(roc, print.thres = 'best')

# Confusion matrix
threshold <- .82
matrix <- round(prop.table(table(lalonde$employed78, 
                                 final_log_fit$fitted > threshold), 1)*100,1)
print('Confusion Matrix')
table(lalonde$employed78,final_log_fit$fitted > threshold)

# Coefficient matrix
coeff <- cbind(exp(summary(final_log_fit)$coefficients[, 1]),
               exp(confint(final_log_fit)))
coeff <- data.frame(coeff) 
names(coeff) <- c('Estimate', '2.5%', '97.5%')
print('Confidence intervals')
coeff
```

## Interpretation

Our model has an AUC of `r round(auc(roc),2)`. Using the suggested threshold of `r threshold`, our model has a sensitivity of `r matrix[2,2]/100` and a specificity of `r 1-(matrix[1,1]/100)`. In other words, our model correctly predicts `r matrix [2,2]`% of nonzero wage earners and `r matrix [1,1]`% of zero wage earners.


**Intercept**: For non-black, non-hispanic, un-married individuals of average age, average 1975 salaries and a zero 1974 salary, with some High School or more education, who did not recieve treatment, we estimate the odds of nonzero salary 1978 to be `r round(coeff['(Intercept)','Estimate'],2)` (95% CI: \$`r round(coeff['(Intercept)','2.5%'],3)`, `r round(coeff['(Intercept)','97.5%'],2)`). 

**Intercept**: For non-black, non-hispanic, un-married individuals of average age, average 1975 salaries and a zero 1974 salary, with some High School or more education, who did not recieve treatment, we estimate the odds of nonzero salary 1978 to be `r round(coeff['(Intercept)','Estimate'],2)` (95% CI: `r round(coeff['(Intercept)','2.5%'],3)`, `r round(coeff['(Intercept)','97.5%'],2)`). 


**Treatment**: Holding all else constant, for individuals who participated in the treatment we estimate the odds of nonzero wage in 1978 to increase by a factor of `r round(coeff['treat1','Estimate'],2)` (95% CI: `r round(coeff['treat1','2.5%'],2)`, `r round(coeff['treat1','97.5%'],2)`).

**Education**: Holding all else contant, for an individual less than a middle school education we estimate e estimate the odds of nonzero wage in 1978 to decrease by a factor of `r round(coeff['educ.bin2MS or less','Estimate'],2)` less (95% CI: `r round(coeff['educ.bin2MS or less','2.5%'],2)`, `r round(coeff['educ.bin2MS or less','97.5%'],2)`). 

**Age**: Holding all else constant, for each 10 years an individual ages on average we estimate his salary to decrease by `r round(exp(log(coeff['agec','Estimate'])*10),2)` (95% CI: `r round(exp(log(coeff['agec','2.5%'])*10),2)`, `r round(exp(log(coeff['agec','97.5%'])*10),2)`).

```{r, echo = FALSE, fig.height=3}
# Create dummy dataset for charting
newval_age <- data.frame(treat = as.factor(0),
                     employed74 = 0,
                     age = seq(min(lalonde$age), max(lalonde$age)),
                     married = 0,
                     black = 0,
                     hispan = 0,
                     educ.bin2 = 'Some HS +',
                     re75c= 0) %>%
    mutate(agec = age - mean(age))

# Predict responses
predict <- predict.glm(final_log_fit, newval_age, interval = 'response', se.fit = TRUE)

# Create confidence interval
t <- 1.96 ## approx 95% CI
upr <- predict$fit + (t * predict$se.fit)
lwr <- predict$fit - (t * predict$se.fit)
fit <- predict$fit

# Append predictions
newval_age <- newval_age %>%
    mutate(fit = exp(fit),
           lwr = exp(lwr),
           upr = exp(upr))

# Print plot
ggplot(data = newval_age) + 
    geom_line(mapping = aes(x = age, y = fit) ) + 
    geom_line(mapping = aes(x = age, y = lwr), color = 'grey') +
    geom_line(mapping = aes(x = age, y = upr),  color = 'grey') +
    ylab('Odds of nonzero salary in 1978') + 
    xlab('Age') +
    ggtitle('Odds of nonzero salary in 1978 by Age') +
    labs(subtitle = 'Confidence Interval')

```

**Married**: Holding all else constant, for married individuals we estimate estimate the odds of nonzero wage in 1978 to increase by a factor of `r round(coeff['married','Estimate'],2)` more (95% CI: `r round(coeff['married','2.5%'],2)`, `r round(coeff['married','97.5%'],2)`). Given that this confidence interval includes 1, we are not confident that there is a meaningful effect of being married on odds of nonzero salary in 1978.

**Black**: Holding all else constant, for black individuals we estimate estimate the odds of nonzero wage in 1978 to decrease by a factor of `r round(coeff['black','Estimate'],2)` less (95% CI: `r round(coeff['black','2.5%'],2)`, `r round(coeff['black','97.5%'],2)`).

**Hispanic**: Holding all else constant, for black individuals we estimate estimate the odds of nonzero wage in 1978 to increase by a factor of `r round(coeff['hispan','Estimate'],2)` more (95% CI: `r round(coeff['hispan','2.5%'],2)`, `r round(coeff['hispan','97.5%'],2)`). Given that this confidence interval includes 1, we are not confident that there is a meaningful effect of hispanic ethnicity on odds of nonzero salary in 1978.

**1975 Salary**: Holding all else constant, for each $1,000 an individual made in 1975, on average we estimate estimate the odds of nonzero wage in 1978 to increase by a factor of `r round(exp(log(coeff['re75c','Estimate'])*1000),2)` higher (95% CI:`r round(exp(log(coeff['re75c','2.5%'])*1000),2)`, `r round(exp(log(coeff['re75c','97.5%'])*1000),2)`). 

```{r, echo = FALSE, fig.height=3}
# Create dummy dataset for charting
newval_75 <- data.frame(treat = as.factor(0),
                     employed74 = 0,
                     agec = 0,
                     married = 0,
                     black = 0,
                     hispan = 0,
                     educ.bin2 = 'Some HS +',
                     re75 = seq(min(lalonde$re75), max(lalonde$re75))) %>%
    mutate(re75c = re75 - mean(re75))

# Predict responses
predict <- predict.glm(final_log_fit, newval_75, interval = 'response', se.fit = TRUE)

# Create confidence interval
t <- 1.96 ## approx 95% CI
upr <- predict$fit + (t * predict$se.fit)
lwr <- predict$fit - (t * predict$se.fit)
fit <- predict$fit

# Append predictions
newval_75 <- newval_75 %>%
    mutate(fit = exp(fit),
           lwr = exp(lwr),
           upr = exp(upr))

# Print plot
ggplot(data = newval_75) + 
    geom_line(mapping = aes(x = re75, y = fit) ) + 
    geom_line(mapping = aes(x = re75, y = lwr), color = 'grey') +
    geom_line(mapping = aes(x = re75, y = upr),  color = 'grey') +
    ylab('Odds of nonzero salary in 1978') + 
    xlab('1975 Salary') +
    ggtitle('Odds of nonzero salary in 1978 by 1975 Salary') +
    labs(subtitle = 'Confidence Interval')

```


**1974 Salary (Zero vs. Nonzero)**:

```{r, echo = FALSE, fig.height=3, warning = FALSE}
# Rebase our models so we can discuss the impacts of 1974 nonzero salary
# Nonzero 1974
lalonde_1974 <- lalonde %>% 
    mutate(employed74 = factor(employed74, levels = c(1,0)))

log_fit_74 <- glm(employed78 ~ treat*employed74 + agec + age2 + age3 + 
                         married + black + hispan + educ.bin2 + re75c, 
                     data = lalonde_1974, family = binomial)
coeff_74 <- cbind(exp(summary(log_fit_74)$coefficients[, 1]),
               exp(confint(log_fit_74)))
coeff_74 <- data.frame(coeff_74) 
names(coeff_74) <- c('Estimate', '2.5%', '97.5%')
```

* Zero Salary: Holding all else constant, for individuals who participated in the treatment we estimate the odds of nonzero wage in 1978 to increase by a factor of `r round(coeff['treat1','Estimate'],2)` (95% CI: `r round(coeff['treat1','2.5%'],2)`, `r round(coeff['treat1','97.5%'],2)`). 

* Nonzero Salary: Holding all else constant, for individuals who participated in the treatment we estimate the odds of nonzero wage in 1978 to decrease by a factor of `r round(coeff_74['treat1','Estimate'],2)` (95% CI: `r round(coeff_74['treat1','2.5%'],2)`, `r round(coeff_74['treat1','97.5%'],2)`). Given that this confidence interval includes 1, we are not confident that there is a meaningful effect of treatment on nonzero salary in 1978 for those with nonzero salaries in 1974.

```{r, echo = FALSE, fig.height=3}
# Create dummy dataset for charting
newval_74 <- data.frame(treat = as.factor(c(0,0,1,1)),
                     employed74 = c(0,1,0,1),
                     agec = 0,
                     married = 0,
                     black = 0,
                     hispan = 0,
                     educ.bin2 = 'Some HS +',
                     re75c = 0)

# Predict responses
predict <- predict.glm(final_log_fit, newval_74, interval = 'response', se.fit = TRUE)

# Create confidence interval
t <- 1.96 ## approx 95% CI
upr <- predict$fit + (t * predict$se.fit)
lwr <- predict$fit - (t * predict$se.fit)
fit <- predict$fit

# Append predictions
newval_74 <- newval_74 %>%
    mutate(fit = exp(fit),
           lwr = exp(lwr),
           upr = exp(upr))

newval_74 %>%
    mutate(employed74 = ifelse(employed74 == 0, 'Zero 1974 Salary', 'Nonzero 1974 Salary')) %>%
    ggplot() + 
    facet_grid(. ~ employed74) +
    geom_point(mapping = aes(x = treat, y = fit) ) + 
    geom_point(mapping = aes(x = treat, y = lwr), shape = 1) +
    geom_point(mapping = aes(x = treat, y = upr), shape = 1) +
    ylab('Odds of nonzero salary in 1978') + 
    xlab('Treament') +
    ggtitle('Interaction effect of nonzero 1974 salary on treatment') + 
    labs(subtitle = 'Confidence Interval')
    

```

## Discussion

According to our findingds, participating in the training program has a positive effect on employment status for individuals who were unemployed in 1974, increasing the odds of a non-zero salary in 1978 by an estimated factor of 2.17. However, we can not say with certainty whether the impact is the same for those who were employed in 1974.

For those employed in 1974 our estimates indicate that the job training program may actually have a negative impact, decreasing odds of employment in 1978 by an estimated factor of .74. There appears to be a negative effect on the odds of employment for those who were employed in 1974, we are not able to say with confidence that this effect is negative.  Moreover, we are unable to say an interaction between treatment and employed status in 1974 exists. The confidence intervals overlap, and as such, the true odds ratio may be the same for both groups.

Odds of employment in 1978 are also influenced by number of years of education, age, and marital status. When all other factors are held constant, each of these factors is associated with greater odds of employment in 1978, with an odds ratio confidence interval that does not include 1.0.

Being black, being hispanic, and an individual's 1975 salary may or may not effect odds of employment in 1978. When all other factors are held constant, each of these factors has an odds ratio confidence interval that includes 1.0.


## Limitations

There are several limitations to our model that should be taken into consideration when interpreting results. First, while the confidence interval for the odds ratio of employment in 1978 for an individual in a job training program doesn't include 1.0, it is very close to 1.0. Therefore, while a positive effect is likely, it may not be large.

Second, our resulting model has a low area under the curve (AUC) of .59. Therefore, an AUC of .59 is on the lower end of the possible range of .5-1.0. Specific to our model, there is a sizable difference in the sensitivity (.820) and specificity (.424) of our model. Meaning, while our model is good at predicting individuals likely to be unemployed in 1978, it is not as good at capturing the breadth of individuals who are truly employed.

# Conclusion

Based on our models, there is good evidence that job training positively influences the odds of employment and the salaries of individuals who complete the program. However, neither our linear model nor our logistic model provide confidence in large increases in odds of employment nor salaries, as they both have large confidence intervals.

# Appendix

## Interaction Exploration

Below is an exploration of potential interaction effects between treatment and other predictor variables in our dataset. Although some exploratory analysis seemed promising,  no addition of interaction terms meaningfully increased our models performance. Therefore our final model did not include any interaction effects.

### Linear Model
```{r, echo = FALSE}
# AGE
i1 <- ggplot(lalonde) +
    geom_point(mapping = aes(x = age, y = re78)) + 
    ylim(c(0,61000)) + 
    facet_grid(. ~ treat)

# EDUC
i3 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = as.factor(educ), y = re78)) + 
    ylim(c(0,61000)) + 
    facet_grid(. ~ treat)

i4 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = educ.bin, y = re78)) + 
    ylim(c(0,61000)) + 
    facet_grid(. ~ treat)


# RE74
i5 <- ggplot(lalonde) +
    geom_point(mapping = aes(x = re74, y = re78)) + 
    ylim(c(0,61000)) + 
    facet_grid(. ~ educ.bin)

# BLACK
i6 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = as.factor(black), y = re78)) + 
    ylim(c(0,61000)) + 
    facet_grid(. ~ treat)

# HISPAN
i7 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = as.factor(hispan), y = re78)) + 
    ylim(c(0,61000)) + 
    facet_grid(. ~ treat)

# MARRIED
i8 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = as.factor(married), y = re78)) + 
    ylim(c(0,61000)) + 
    facet_grid(. ~ treat)

# NODEGREE
i9 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = as.factor(nodegree), y = re78)) + 
    ylim(c(0,61000)) + 
    facet_grid(. ~ treat)

grid.arrange(i1, i3, i4, i5, i6, i7, i8, i9,
             top = 'Potential Interaction Effects: Linear Model')
```

### Logistic Model

Below is an exploration of potential interaction effects between treatment and other predictor variables in our dataset. The only interaction effect that meaningfully improved prediction for our model was the addition an interaction between a nonzero employement in 1974 and treatment.

```{r, echo = FALSE}
par(mfrow = c(1,2))
# Treatment and 74 salary
# No reason for interaction effect
binnedplot(x = lalonde[lalonde$treat == 0, 're74'], y = lalonde[lalonde$treat == 0, 'employed78'],
           xlab = 'Re74', ylab = 'employed78 \'78 Cases',
           ylim = c(.6,1),
           xlim = c(0,20000),
           main = 'No Treatment')
binnedplot(x = lalonde[lalonde$treat == 1, 're74'], y = lalonde[lalonde$treat == 1, 'employed78'],
           xlab = 'Re74', ylab = 'employed78 \'78 Cases',
           ylim = c(.6,1),
           xlim = c(0,20000),
           main = 'Treatment')

# Treatment and 75 salary
# No reason for interaction effect
binnedplot(x = lalonde[lalonde$treat == 0, 're75'], y = lalonde[lalonde$treat == 0, 'employed78'],
           xlab = 'Re75', ylab = 'employed78 \'78 Cases', 
           ylim = c(.6,1),
           xlim = c(0,20000),
           main = 'No Treatment')
binnedplot(x = lalonde[lalonde$treat == 1, 're75'], y = lalonde[lalonde$treat == 1, 'employed78'],
           xlab = 'Re75', ylab = 'employed78 \'78 Cases',
           ylim = c(.6,1),
           xlim = c(0,20000),
           main = 'Treatment')

# Binary 75 employed
# Treatment doesn't have an effect for those who were in employed in 75
# but does have a significant effect if unemployed in 74
tab <- rbind(tapply(lalonde[lalonde$treat == 0, 'employed78'], lalonde[lalonde$treat == 0, 'employed75'], mean),
             tapply(lalonde[lalonde$treat == 1, 'employed78'], lalonde[lalonde$treat == 1, 'employed75'], mean))
rownames(tab) <- c('No Treatment', 'Treatment')
colnames(tab) <- c('Unemployed 75', 'Employed 75')
tab

# Binary 74 employed
# Interaction effect that might wash out when we add 75 interaction
tab <- rbind(tapply(lalonde[lalonde$treat == 0, 'employed78'], lalonde[lalonde$treat == 0, 'employed74'], mean),
             tapply(lalonde[lalonde$treat == 1, 'employed78'], lalonde[lalonde$treat == 1, 'employed74'], mean))
rownames(tab) <- c('No Treatment', 'Treatment')
colnames(tab) <- c('Unemployed 74', 'Employed 74')
tab


# Treatment and educ.bin2
# No reason for interaction effect
# Might help those with lower education
tab <- rbind(tapply(lalonde[lalonde$treat == 0, 'employed78'], lalonde[lalonde$treat == 0, 'educ.bin2'], mean),
             tapply(lalonde[lalonde$treat == 1, 'employed78'], lalonde[lalonde$treat == 1, 'educ.bin2'], mean))
rownames(tab) <- c('No Treatment', 'Treatment')
colnames(tab) <- c('Some HS +', 'MS or less')
tab
```



## Residuals

### Linear Model

```{r, echo = FALSE}
# TREAT
r1 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = treat, y = final_linear_fit$residuals)) + 
    ylab('Residuals')

# AGE
r2 <- ggplot(lalonde) +
    geom_point(mapping = aes(x = age, y = final_linear_fit$residuals)) + 
    ylab('Residuals') 

# EDUC
r3 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = educ.bin, y = final_linear_fit$residuals)) + 
    ylab('Residuals')

# BLACK
r4 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = as.factor(black), y = final_linear_fit$residuals)) + 
    ylab('Residuals') 

# HISPAN
r5 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = as.factor(hispan), y = final_linear_fit$residuals)) + 
    ylab('Residuals')

# MARRIED
r6 <- ggplot(lalonde) +
    geom_boxplot(mapping = aes(x = as.factor(married), y = final_linear_fit$residuals)) + 
    ylab('Residuals')

# RE74
r7 <- ggplot(lalonde) +
    geom_point(mapping = aes(x = re74, y = final_linear_fit$residuals)) + 
    ylab('Residuals')

# RE75
r8 <- ggplot(lalonde) +
    geom_point(mapping = aes(x = re75, y = final_linear_fit$residuals)) + 
    ylab('Residuals') 

grid.arrange(r1, r2, r3, r4, r5, r6, r7, r8,
             top = 'Linear Model Residuals')
```


### Logistic Model

```{r, echo = FALSE}
# Calculate logistical model residuals
rawreds <- lalonde$employed78 - fitted(final_log_fit)

# educ.bin2
tapply(rawreds, lalonde$educ.bin2, mean) 

# black
tapply(rawreds, lalonde$black, mean) 

# hispan
tapply(rawreds, lalonde$hispan, mean) 

# married
tapply(rawreds, lalonde$married, mean)

# employed74
tapply(rawreds, lalonde$employed74, mean) 

# AGE
# add a third-degree poly?
binnedplot(x = lalonde$age, y = rawreds,
           xlab = 'Age', ylab = 'Residuals')
```


## Influential Points

### Linear Model
Observations with high leverage or cooks distance in our final linear model are below:
```{r influential, echo = FALSE, fig.align='left'}
library(MASS)
# Calcate leveage and cooks distance for each observation
leverage = hatvalues(final_linear_fit)
cooks = cooks.distance(final_linear_fit)

# Append leverage and cooks to our data
leverage <- lalonde %>%
    mutate(leverage, cooks)

# Plot leverage vs. id
l <- ggplot(data = leverage) +
    geom_point(mapping = aes(x = seq(nrow(lalonde)), y = leverage)) +
    labs(subtitle = 'Leverage')
# Plot cooks vs. id
c <- ggplot(data = leverage) +
    geom_point(mapping = aes(x = seq(nrow(lalonde)), y = cooks)) +
    labs(subtitle = 'High Cooks Distance')

grid.arrange(l, c, top = 'Potentially Influential Points')

# Take a look at the potentially influential points
leverage %>%
    filter(leverage > .05 | cooks > .02)
```

The influential points show that our model is not as accurate in its predictions for those who have high salaries in either 1974 or 1975. Because these are not the typical demographic to partake in a job training program, they are not of great interest for this research paper. Therefore we do not alter our model. 

### Logistic Model

Observations with high leverage or cooks distance in our final logistic model are below:
```{r influential2, echo = FALSE, fig.align='left'}
library(MASS)
# Calcate leveage and cooks distance for each observation
leverage = hatvalues(final_log_fit)
cooks = cooks.distance(final_log_fit)

# Append leverage and cooks to our data
leverage <- lalonde %>%
    mutate(leverage, cooks)

# Plot leverage vs. id
l <- ggplot(data = leverage) +
    geom_point(mapping = aes(x = seq(nrow(lalonde)), y = leverage)) +
    labs(subtitle = 'Leverage')
# Plot cooks vs. id
c <- ggplot(data = leverage) +
    geom_point(mapping = aes(x = seq(nrow(lalonde)), y = cooks)) +
    labs(subtitle = 'High Cooks Distance')

grid.arrange(l, c, top = 'Potentially Influential Points')

# Take a look at the potentially influential points
leverage %>%
    filter(leverage > .04 | cooks > .012)
```

We see that many of influential points are those who have nonzero salary in 1974, but who have a zero salary in 1978. These are, in fact, outliers in our observation as they are people who seem to be meaningfully financially worse off in 1978 than in 1974. These cases are potentially influencial in our model, but are scientifically relavent cases to include in our observations. Because we cannot justify removing these observations from our dataset, we leave our model unaltered.

Additionally, there are several outliers including relatively high 1978 salaries. Again, although this observation is not the norm, we include this observation in our model as it is an important edge case.
